services:
  spark-master:
    image: spark:3.5.4-scala2.12-java11-python3-ubuntu
    container_name: spark-master
    hostname: spark-master
    command: ["/opt/spark/bin/spark-class","org.apache.spark.deploy.master.Master","--host","spark-master","--port","7077","--webui-port","8080"]
    ports: ["7077:7077","8080:8080"]
    environment:
      - SPARK_SUBMIT_OPTS=-Dspark.jars.ivy=/opt/spark/.ivy2
    networks: [lakehouse]
    volumes:
      - ./scala:/work/scala
      - ./data:/work/data
      - ./jars:/work/jars
      - spark-ivy:/opt/spark/.ivy2

  spark-worker:
    image: spark:3.5.4-scala2.12-java11-python3-ubuntu
    container_name: spark-worker
    hostname: spark-worker
    depends_on: [spark-master]
    command: ["/opt/spark/bin/spark-class","org.apache.spark.deploy.worker.Worker","spark://spark-master:7077","--cores","2","--memory","2g","--webui-port","8081"]
    ports: ["8081:8081"]
    environment:
      - SPARK_SUBMIT_OPTS=-Dspark.jars.ivy=/opt/spark/.ivy2
    networks: [lakehouse]
    volumes:
      - ./data:/work/data
      - spark-ivy:/opt/spark/.ivy2

  jupyter:
    image: jupyter/pyspark-notebook:python-3.11
    container_name: lakehouse-jupyter
    depends_on: [spark-master, minio]
    ports: ["8888:8888"]
    environment:
      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}
      - MLFLOW_S3_ENDPOINT_URL=${MLFLOW_S3_ENDPOINT_URL}
      - AWS_DEFAULT_REGION=us-east-1
      - AWS_EC2_METADATA_DISABLED=true
      - AWS_S3_FORCE_PATH_STYLE=true
      - PYSPARK_PYTHON=python
      - PYSPARK_DRIVER_PYTHON=jupyter
      - PYSPARK_DRIVER_PYTHON_OPTS=lab --NotebookApp.token='' --NotebookApp.password=''
    
    working_dir: /home/jovyan/work
    command: >
      bash -lc "pip install --no-cache-dir delta-spark==3.1.0 great_expectations mlflow boto3 &&
                start-notebook.sh --NotebookApp.token=''"
    networks: [lakehouse]
    volumes:
      - ./py:/home/jovyan/work/py
      - ./data:/home/jovyan/work/data
      - ./scripts:/home/jovyan/work/scripts

      # - ./py:/work/py
      # - ./data:/work/data
      # - ./scripts:/work/scripts 

  minio:
    image: minio/minio:latest
    container_name: minio
    env_file: .env
    command: server /data --console-address ':9001'
    ports: ["9000:9000","9001:9001"]
    volumes:
      - ./minio/data:/data
    networks: [lakehouse]

  minio-setup:
    image: minio/mc:latest
    container_name: minio-setup
    depends_on: [minio]
    env_file: .env
    entrypoint: >
      /bin/sh -c "
      set -e;
      until mc alias set minio http://minio:9000 $${MINIO_ROOT_USER} $${MINIO_ROOT_PASSWORD}; do
        echo 'waiting for MinIO...'; sleep 2;
      done;
      for b in bronze silver gold mlflow; do mc mb -p minio/$$b || true; done;
      mc ls minio
      "
    networks: [lakehouse]

  mlflow:
    image: python:3.11-slim
    container_name: mlflow
    depends_on: [minio]
    env_file: .env
    environment:
      # Make sure these exist in your .env (or set here)
      AWS_ACCESS_KEY_ID: ${AWS_ACCESS_KEY_ID:-minioadmin}
      AWS_SECRET_ACCESS_KEY: ${AWS_SECRET_ACCESS_KEY:-minioadmin}
      MLFLOW_S3_ENDPOINT_URL: ${MLFLOW_S3_ENDPOINT_URL:-http://minio:9000}
      AWS_REGION: ${AWS_REGION:-us-east-1}
      AWS_EC2_METADATA_DISABLED: "true"
    working_dir: /mlflow
    volumes:
      - ./mlflow:/mlflow
    ports: ["5000:5000"]
    command:
      [
        "bash","-lc",
        "pip install --no-cache-dir mlflow boto3 && \
         exec mlflow server \
          --backend-store-uri sqlite:///mlruns.db \
          --default-artifact-root s3://mlflow \
          --host 0.0.0.0 --port 5000 \
          --allowed-hosts '*' --cors-allowed-origins '*'"
      ]
    healthcheck:
      test: ["CMD", "python", "-c", "import urllib.request as u; u.urlopen('http://127.0.0.1:5000/').read(1)"]
      interval: 10s
      timeout: 3s
      retries: 20
    networks: [lakehouse]



  scala-builder:
    image: sbtscala/scala-sbt:eclipse-temurin-jammy-11.0.21_9_1.9.7_2.12.18
    container_name: scala-builder
    profiles: ["build"]
    working_dir: /work
    command: sbt -v clean package
    volumes:
      - ./scala:/work
    networks: [lakehouse]

networks:
  lakehouse:
    driver: bridge

volumes:
  spark-ivy:

